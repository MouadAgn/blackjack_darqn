# ğŸƒ Blackjack Atari Agent - DARQN + Attention

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange)
![Gymnasium](https://img.shields.io/badge/Gymnasium-Atari-green)
![Status](https://img.shields.io/badge/Status-Training-yellow)

> Un agent d'Apprentissage par Renforcement Profond (Deep RL) capable de jouer au Blackjack sur l'environnement Atari (`ALE/Blackjack-v5`).

## ğŸ“º DÃ©mo (Attention Map)

Voici ce que l'agent "regarde" pendant qu'il joue. La carte de chaleur (heatmap) rouge indique les zones d'attention du rÃ©seau de neurones :

![Demo Blackjack](blackjack_attention.gif)

*(Si le GIF ne s'affiche pas, assurez-vous d'avoir lancÃ© `test.py` pour le gÃ©nÃ©rer)*

## ğŸ§  Architecture du ModÃ¨le

Ce projet utilise une architecture **DARQN (Deep Attention Recurrent Q-Network)**. Contrairement Ã  un DQN classique, ce modÃ¨le est conÃ§u pour les environnements oÃ¹ l'information est partielle ou nÃ©cessite une mÃ©moire Ã  court terme.



### Pourquoi cette architecture ?
Le Blackjack n'est pas seulement visuel, il est **sÃ©quentiel**.
1.  **CNN (Convolutional Neural Network)** : Traite l'image brute (pixels) pour extraire les caractÃ©ristiques visuelles.
2.  **Attention Mechanism** : Permet au rÃ©seau de se focaliser uniquement sur les cartes et d'ignorer le fond dÃ©coratif du casino Atari.
3.  **LSTM (Long Short-Term Memory)** : Retient l'historique de la main (quelles cartes ont dÃ©jÃ  Ã©tÃ© tirÃ©es) pour prendre une dÃ©cision Ã©clairÃ©e (Hit ou Stick).

## ğŸ“‚ Structure du Projet

```text
blackjack_darqn/
â”‚
â”œâ”€â”€ checkpoints/             # Sauvegarde des poids du modÃ¨le (.pth)
â”œâ”€â”€ logs/                    # Logs d'entraÃ®nement
â”‚
â”œâ”€â”€ src/                     # Code source
â”‚   â”œâ”€â”€ model.py             # Le rÃ©seau (CNN + Attention + LSTM)
â”‚   â”œâ”€â”€ memory.py            # Replay Buffer SÃ©quentiel
â”‚   â”œâ”€â”€ agent.py             # L'agent (SÃ©lection d'action & EntraÃ®nement)
â”‚   â””â”€â”€ utils.py             # Wrappers & Preprocessing Atari
â”‚
â”œâ”€â”€ config.py                # HyperparamÃ¨tres
â”œâ”€â”€ main.py                  # Script d'entraÃ®nement
â”œâ”€â”€ test.py                  # Script de visualisation
â””â”€â”€ requirements.txt         # DÃ©pendances
```

âš™ï¸ Installation
1. PrÃ©requis
Assurez-vous d'avoir Python 3.8+ installÃ©.

2. Installation des dÃ©pendances
Installez les bibliothÃ¨ques nÃ©cessaires, y compris les ROMs Atari :

Bash

pip install -r requirements.txt
Contenu du requirements.txt suggÃ©rÃ© :

Plaintext

gymnasium[atari, accept-rom-license]
torch
torchvision
numpy
opencv-python
imageio
3. PrÃ©paration des dossiers
CrÃ©ez les dossiers pour stocker les sauvegardes si ce n'est pas fait :

Bash

mkdir checkpoints
mkdir logs
ğŸš€ Utilisation
1. EntraÃ®nement de l'Agent (main.py)
Pour lancer l'apprentissage depuis zÃ©ro. L'agent va explorer l'environnement, remplir sa mÃ©moire et apprendre via le DARQN.

Bash

python main.py
Les modÃ¨les seront sauvegardÃ©s automatiquement dans checkpoints/ tous les X Ã©pisodes (ex: model_100.pth).

Note : L'entraÃ®nement sur pixels est long. Laissez tourner plusieurs heures pour obtenir des rÃ©sultats probants.

2. Test et Visualisation (test.py)
Ce script charge un modÃ¨le entraÃ®nÃ©, joue une partie et gÃ©nÃ¨re un GIF montrant ce que l'IA "regarde" grÃ¢ce Ã  la carte d'attention.

Ouvrez test.py.

Modifiez la ligne de chargement avec votre fichier .pth :

Python

# Exemple
run_test("checkpoints/model_1000.pth")
Lancez le script :

Bash

python test.py
Le rÃ©sultat sera sauvegardÃ© dans le fichier blackjack_attention.gif.